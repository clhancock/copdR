---
title: "howto_copdr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{howto_copdr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, results='hide'}
#library(copdr)
devtools::load_all()
```


# Introduction to Cumulative Offset Probability Distribution in R (copdr)

This code is meant to import probability distribution functions (PDFs), i.e. Gaussian curves, and combine them in a modicum of ways for the user, based on several filters to take out the noisy PDFs or PDFs with high uncertainties (i.e. large standard deviations, high coefficient of variation, etc). These PDFs are produced from LaDiCaoz, a Matlab program that allows you to 'backslip' offset on a fault from digital elevation models (DEMs) or digital surface models (DSMs). It is a lot of acronyms, I know. I will simplify this description in a future version. 

#CH I don't think this overly complicated. Maybe you could references to some articles that use the method? Or provide a link to the matlab program documentation? You could also provide some background to the "why" you wanted this package. Is it just for plotting these results? Why is using R better than matlab?

# 1. Load in data

First, you need to make sure that the dataset is in a folder in your working directory.
The first part of the code does the following:

1. Generates a reference vector of folder names, from the files in the dataset folder. Currently, this code only works to process .mat files. After running this first section of code, you will be prompted to enter the name of the folder containing all the .mat files. This does NOT need quotations around it.
2. Imports the metadata from a reference .csv file. See "example.csv" or "example.xlsx" for how this reference metadata should be formatted.

## User-defined inputs ##
```{r}
#First, set the directory where all your data exist.
directory <- system.file("extdata", package = "copdr") #CHWhy not have a data folder?
folder_name <- "example_mat"
csv_name    <- "example.csv"
```

#CH use hide to tidy up the vignette
```{r,results='hide',cache=TRUE}
# list.mat() takes the folder ' example_mat ' in a directory and lists all the file names with the file format '.mat'
backslip_names <- list.mat(directory, folder_name)

#create a list containing three dataframes, including 1. lateral, 2. vertical, 3. total offset
backslip_data <- matrix.mat(directory,folder_name,backslip_names)

#load in your reference CSV. See example table "example.csv" for how your table should be formatted
backslip_csv <- readr::read_csv(file.path(directory,csv_name), show_col_types = FALSE)
```

```{r}
print(head(backslip_names))
print(head(backslip_csv))
```



# 2. Data pre-processing

## 2.1. Visualize raw data

This code plots your data on a default coordinate axis, to check that the data loaded in right.
```{r,message=FALSE,fig.width=6.5}
## DO NOT EDIT ##
#prep data for plotting
raw_lat_data <- simplify4plot(backslip_data[[1]], "lateral") #CH what do the column names mean?
raw_vert_data <- simplify4plot(backslip_data[[2]], "vertical")
raw_total_data <- simplify4plot(backslip_data[[3]], "total")

# plot raw data
plotMatRevise(raw_lat_data,"lateral")
plotMatRevise(raw_vert_data,"vertical")
plotMatRevise(raw_total_data,"total")
```

## 2.2 Clip raw data and plot

Use the graphs from 2.1 to clip your dataset.
```{r,message=FALSE,fig.width=6.5}

## USER-DEFINED LIMITS ##
lat_limits <- list(
     lat_x_min = -5,
     lat_x_max = 0,
     lat_y_min = 0,
     lat_y_max = 1
)

vert_limits <- list(
     vert_x_min = -1,
     vert_x_max = 0,
     vert_y_min = 0,
     vert_y_max = 1
)

total_limits <- list(
     total_x_min = 0,
     total_x_max = 5,
     total_y_min = 0,
     total_y_max = 1
)  

#######################

## DO NOT EDIT ##

#clip x limits of data
backslip_data_clip <- list(
     backslip_lat = data.clip(backslip_data[[1]],"lateral",lat_limits[[1]],lat_limits[[2]]),
     backslip_vert = data.clip(backslip_data[[2]],"vertical",vert_limits[[1]],vert_limits[[2]]),
     backslip_tot = data.clip(backslip_data[[3]],"total",total_limits[[1]],total_limits[[2]])
)

# plot clipped data
plotMat(simplify4plot(backslip_data_clip[[1]], "lateral"),"lateral",
        lat_limits[[1]],lat_limits[[2]],lat_limits[[3]],lat_limits[[4]])
plotMat(simplify4plot(backslip_data_clip[[2]], "vertical"),"vertical",
        vert_limits[[1]],vert_limits[[2]], vert_limits[[3]],vert_limits[[4]])
plotMat(simplify4plot(backslip_data_clip[[3]], "total"),"total",
        total_limits[[1]],total_limits[[2]],total_limits[[3]],total_limits[[4]])
```

## 2.3 Confidence Scaling

This code scales each PDF by a confidence value. These confidence values are specific to the dataset, where 1 is the maximum confidence and your largest value is the lowest confidence. In this dataset, 3 is the lowest confidence. The code is set up to accommodate any confidence scale (1 to 3, 1 to 5, 1 to 10, etc). If your confidence values are flipped in the reference metadata, such that 1 is the lowest confidence value, you will need to invert the values in your metadata.
```{r}
## USER DEFINED INPUT ##
#column of csv table corresponding to confidence values
confidence_column = 9
N_observations = 205
########################




## DO NOT EDIT ##

#scale the data by confidence values and put in a new list
backslip_data_Cscale <- backslip_data
for(i in 1:3){
     backslip_data_Cscale[[i]] <- conf.scale(backslip_data_clip[[i]],backslip_csv, confidence_column, N_observations)
}

# plot confidence-scaled data
plotMat(simplify4plot(backslip_data_Cscale[[1]], "lateral"),"lateral",
        lat_limits[[1]],lat_limits[[2]],lat_limits[[3]],lat_limits[[4]])
plotMat(simplify4plot(backslip_data_Cscale[[2]], "vertical"),"vertical",
        vert_limits[[1]],vert_limits[[2]], vert_limits[[3]],vert_limits[[4]])
plotMat(simplify4plot(backslip_data_Cscale[[3]], "total"),"total",
        total_limits[[1]],total_limits[[2]],total_limits[[3]],total_limits[[4]])
##################
```

## 2.4 Coefficient of variation (COV) calculation

#the master reference CSV "backslip_csv" should have 4-6 columns for displacement statistics
# 1. lateral mean, 2. lateral standard deviation, 3. vertical mean, 4. vertical standard deviation,
# 5. total mean, 6. total standard devation
# for all offset sites

#inputs will need to be the columns of offset data

#the following code calculates the coefficient of variation, 
#which is the standard deviation divided by the mean
#I used lateral in my example because in the example data,
#vertical displacement is poorly resolved and small magnitude

```{r}


## USER DEFINED INPUT ##
# these are the column numbers corresponding to this data in the reference csv
lat_mean = 13
lat_stdev = 14
vert_mean = 15
vert_stdev = 16
total_mean = 17
total_stdev = 18

########################
     


## DO NOT EDIT ##

backslip_csv <- calc.cov(backslip_csv,"lateral", lat_mean, lat_stdev)
backslip_csv <- calc.cov(backslip_csv,"vertical",vert_mean, vert_stdev)
backslip_csv <- calc.cov(backslip_csv,"total",total_mean, total_stdev)
########################
```


# 3. Filters 

- these filters will work to clip master data matrices to include only file names that meet a set of criteria
- users will define these criteria and the code will filter through the CSV and data matrices to remove
- PDFs that do not fit these criteria

- Each filter will essentially clip the data and produce an output of PDFs that can be plotted, which can then be fed into any of the other filters

3.1 COV filters
```{r}
#these are the COV values you want to filter for
COV_filter_vals <- seq(.10,.30, by=0.05) 



## DO NOT EDIT ##
# this creates reference data files for the offset_IDs and the corresponding COV values
lat_covREF <- filter_covREF(backslip_csv,1,"lateral",COV_filter_vals)
vert_covREF <- filter_covREF(backslip_csv,1,"vertical",COV_filter_vals)
total_covREF <- filter_covREF(backslip_csv,1,"total",COV_filter_vals)

### this filters the input datatable 'backslip_data' by the filtered COV reference files created using filter.covREF
lat_cov2 <- filter_covDAT(backslip_csv,1,"lateral",COV_filter_vals,lat_covREF,backslip_data_Cscale)
vert_cov2 <- filter_covDAT(backslip_csv,1,"vertical",COV_filter_vals,vert_covREF,backslip_data_Cscale)
total_cov2 <- filter_covDAT(backslip_csv,1,"total",COV_filter_vals,total_covREF,backslip_data_Cscale)
########################

# plot confidence-scaled data
plotMatSum(lat_cov2[[5]] ,"lateral", lat_limits[[1]], lat_limits[[2]])
plotMatSum(vert_cov2[[5]] ,"vertical", vert_limits[[1]],vert_limits[[2]])
plotMatSum(total_cov2[[5]] ,"total", total_limits[[1]],total_limits[[2]])
##################
```

